<!DOCTYPE html>
<html>
<head>
    <title>Vincent Sitzmann</title>

    <!-- Meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Styles -->
    <style>
          body {
            font-family: Roboto, 'sans-serif';
            font-size: 16px;
            background-color: #FFFFFF;
            color: #4F6071;
          }
          h1 {
              font-weight: 300;
              font-size: 2rem;
          }
          #header {
            background-color: #f4f4f4;
            /*background-color: #FFFFFF;*/
            display: flex;
            align-items: flex-end;
            padding-top:60px;
            padding-bottom:60px;
          }
          #footer {
            background-color: #FFFFFF;
            padding:60px;
          }
          #portrait {
            border: 3px solid white;
          }
          #header-text {
            margin-top: 60px;
            margin-left: 220px;
          }
          #header-text-name {
            font-size: 40px;
          }
          #header-text-title {
              font-size: 17px;
          }
          #header-text-affiliation {
              font-size: 17px;
              margin-top: 10px;
          }
          #header-text-email {
            margin-top: 10px;
            font-size: 17px;
            font-style: italic;
          }
          .header-text-desc {
            font-size: 20px;
          }
          .vspace-top {
            margin-top: 30px;
          }
          .vspace-top-news {
              margin-top: 15px;
          }
          .paper-image {
            width: 150px;
          }
          .news-date {
              font-weight: bold;
          }
          .paper-title {
            font-weight: bold;
          }
          .paper-authors {
            font-style: italic;
          }
    </style>
</head>

<body>
    <div id='header'>
        <div class='container'>
            <div class='row'>
                <div class="col-sm-3 offset-sm-1">
                    <img src='imgs/portrait.jpeg' class='img-fluid' id='portrait'>
                </div>

                <div class="col">
                  <div id='header-text-name'>
                      Vincent Sitzmann
                  </div>
                    <div id='header-text-title'>
                        Assistant Professor </br>
                        Principal investigator, <a href="https://www.scenerepresentations.org">Scene Representation Group</a>
                    </div>
                    <div id='header-text-affiliation'>
                        MIT Department of <a href="https://www.eecs.mit.edu/">Electrical Engineering & Computer Science</a></br>
                        <a href="https://www.csail.mit.edu/">Computer Science and Artificial Intelligence Laboratory</a> (CSAIL)
                    </div>
                  <div id='header-text-email'>
                        sitzmann (at) mit (dot) edu
                  </div>
                  <div>
                    <a href="https://github.com/vsitzmann">[GitHub]</a>
                    <a href="https://scholar.google.com/citations?user=X44QVV4AAAAJ&hl=en&oi=ao">[Google Scholar]</a>
                    <a href="https://linkedin.com/in/vincentsitzmann">[LinkedIn]</a>
                    <a href="https://twitter.com/vincesitzmann">[Twitter]</a>
                    <!-- <a href="https://sigmoid.social/@vsitzmann">[Mastodon]</a> -->
                    <a href="docs/cv_vincent_sitzmann.pdf">[Download CV]</a>
                  </div>

                </div>
            </div>
        </div>
    </div>


    <div class='container'>
        <div class='row vspace-top'>
            <div class='col offset-sm-1'>
                <h1>Bio</h1>
                <p>
                    I am an Assistant Professor at MIT EECS, where I am leading the
                    <a href="https://www.scenerepresentations.org">Scene Representation Group</a>. 
                    Previously, I did my Ph.D. at Stanford University as well as a Postdoc at MIT CSAIL. 
                    My research interest lies in building AI that perceives and models the world the way that humans do. 
                    Specifically, I work towards models that can learn to reconstruct a rich state description of their 
                    environment, such as reconstructing its 3D structure, materials, semantics, etc. from vision. 
                    These models should also be able to model the impact of their own actions on that environment, 
                    i.e., learn a "mental simulator" or "world model". I am particularly interested in models that 
                    can learn these skills fully self-supervised only from video and by self-directed interaction with the world. 
                </p>

<!--                <h1>Looking for graduate students!</h1>-->
<!--                <p>-->
<!--                    I am looking for graduate students to join my lab at MIT in July 2023. If you want to push what's-->
<!--                    possible with neural scene representations, inverse graphics and neural rendering-->
<!--                    and apply them to problems across computer vision, graphics, and robotics, please apply-->
<!--                    <a href="https://gradapply.mit.edu/eecs/apply/login/">here</a> - -->
<!--                    deadline is December 15th 2022!-->
<!--                </p>-->

<!--                <div class='vspace-top'>-->
<!--                    <h1>News</h1>-->
<!--                </div>-->

<!--                &lt;!&ndash;- List of news -&#45;&#45;!>-->
<!--                <div class='row vspace-top-news'>-->
<!--                    <div class="col-sm-2 news-date">-->
<!--                        March 2022-->
<!--                    </div>-->
<!--                    <div class="col">-->
<!--                        My group website is now online - check i to -->
<!--                    </div>-->
<!--                    -->
<!--                    <div class="col-sm-2 news-date">-->
<!--                        June 2021-->
<!--                    </div>-->
<!--                    <div class="col">-->
<!--                        I am thrilled to announce that I will be joining MIT as tenure-track assistant professor in July-->
<!--                        2022! My lab will investigate neural scene representations, inverse graphics,-->
<!--                        neural rendering, and their applications in vision, graphics, robotics, and AI.-->
<!--                    </div>-->
<!--                </div>-->

<!--                <div class='row vspace-top-news'>-->
<!--                    <div class="col-sm-2 news-date">-->
<!--                        June 2021-->
<!--                    </div>-->
<!--                    <div class="col">-->
<!--                        I have created a slideshare account and will start uploading slides for some of my presentations-->
<!--                        / talks / courses, starting with the slides for the introduction to novel view synthesis at SIGGRAPH-->
<!--                        2021. Feel free to re-use them - I only ask that you keep some form of acknowledgement :) Find them-->
<!--                        <a href="https://www.slideshare.net/VincentSitzmann/">here.</a>-->
<!--                    </div>-->
<!--                </div>-->

<!--                <div class='row vspace-top-news'>-->
<!--                    <div class="col-sm-2 news-date">-->
<!--                        January 2021-->
<!--                    </div>-->
<!--                    <div class="col">-->
<!--		    	I published a <a href="https://github.com/vsitzmann/awesome-implicit-representations">-->
<!--                        reading list on neural implicit representations</a> on github that I give students to get started in this area, inspired by the awesome-computer-vision list with extra commentary &amp; notes. Check it out!-->
<!--                    </div>-->
<!--                </div>-->

<!--                <div class='row vspace-top-news'>-->
<!--                    <div class="col-sm-2 news-date">-->
<!--                        January 2021-->
<!--                    </div>-->
<!--                    <div class="col">-->
<!--		    I am now serving as an academic advisor to <a href="https://preferred.jp/en/news/pr20210113/">Preferred Networks, Inc</a>!-->
<!--                    </div>-->
<!--                </div>-->

<!--                <div class='row vspace-top-news'>-->
<!--                    <div class="col-sm-2 news-date">-->
<!--                        June 2020-->
<!--                    </div>-->
<!--                    <div class="col">-->
<!--                        I just graduated Stanford with my <a href="docs/self_supervised_scene_rep_learning_vsitzmann.pdf">-->
<!--                        thesis on Self-supervised Scene Representation Learning</a>. There's a few interesting thoughts in there - -->
<!--                        especially check out the introduction and conclusion!-->
<!--                    </div>-->
<!--                </div>-->

<!--                <div class='row vspace-top-news'>-->
<!--                    <div class="col-sm-2 news-date">-->
<!--                        March 2020-->
<!--                    </div>-->
<!--                    <div class="col">-->
<!--                        Our CVPR tutorial on Neural Rendering is on youtube, free to watch for everyone!-->
<!--                        Here's the <a href="https://www.youtube.com/watch?v=LCTYRqW-ne8">link to the morning session</a> - -->
<!--                        at 2:20:00, I'm giving an overview over Novel View Synthesis.-->
<!--                        Here's the <a href="https://www.youtube.com/watch?v=JlyGNvbGKB8">link to the afternoon session</a>.-->
<!--                    </div>-->
<!--                </div>-->

<!--                <div class='row vspace-top-news'>-->
<!--                    <div class="col-sm-2 news-date">-->
<!--                        November 2019-->
<!--                    </div>-->

<!--                    <div class="col">-->
<!--		    	Our paper "Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations" wins an honorable mention for "Outstanding New Directions" at NeurIPS 2019! Watch my talk <a href="https://slideslive.com/38921749/track-1-session-3">here</a>.-->
<!--                    </div>-->
<!--                </div>-->


<!--                <div class='row vspace-top-news'>-->
<!--                    <div class="col-sm-2 news-date">-->
<!--                        May 2019-->
<!--                    </div>-->

<!--                    <div class="col">-->
<!--                        I will join Prof. Noah Snavely's group at the Google NYC office over the summer and continue working-->
<!--                        on  deep learning for scene understanding and novel view synthesis.-->
<!--                    </div>-->
<!--                </div>-->


                <div class='vspace-top'>
                    <h1>Publications</h1>
                </div>

                <!--- List of publications ---!>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="imgs/jacobian_fields_teaser.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Unifying 3D Representation and Control of Diverse Robots with a Single Camera
                        </div>
                        <div class='paper-desc'>
                            arXiv
                        </div>
                        <div class='paper-authors'>
                            Sizhe Lester Li, Annan Zhang, Boyuan Chen, Hanna Matusik, Chao Liu, Daniela Rus, Vincent Sitzmann
                        </div>
                        <div>
                            <a href="https://sizhe-li.github.io/publication/neural_jacobian_field/">[Project page]</a>
                            <a href="https://github.com/sizhe-li/neural-jacobian-field">[Code]</a>
                            <a href="https://www.youtube.com/watch?v=dFZ1RvJMN7A&embeds_referring_euri=https%3A%2F%2Fsizhe-li.github.io%2F&source_ve_path=Mjg2NjY">[Video]</a>
                            <a href="https://arxiv.org/abs/2407.01392">[Paper]</a>
                            <a href="https://x.com/vincesitzmann/status/1821576713639510425">[Twitter Thread]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="imgs/diff_forcing_teaser.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion
                        </div>
                        <div class='paper-desc'>
                            NeurIPS
                        </div>
                        <div class='paper-authors'>
                            Boyuan Chen*, Diego Marti Monso*, Yilun Du, Max Simchowitz, Russ Tedrake, Vincent Sitzmann
                        </div>
                        <div>
                            <a href="https://boyuan.space/diffusion-forcing/">[Project page]</a>
                            <a href="https://github.com/buoyancy99/diffusion-forcing">[Code]</a>
                            <a href="https://arxiv.org/abs/2407.01392">[Paper]</a>
                            <a href="https://x.com/BoyuanChen0/status/1808538170067407264">[Twitter Thread]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/niso_teaser.png' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Neural Isometries: Taming Transformations for Equivariant ML
                        </div>
                        <div class='paper-desc'>
                            NeurIPS
                        </div>
                        <div class='paper-authors'>
                            Tommy Mitchel, Michael Taylor, Vincent Sitzmann
                        </div>
                        <div>
                            <!-- <a href="https://github.com/buoyancy99/diffusion-forcing">[Code]</a> -->
                            <a href="https://arxiv.org/abs/2405.19296">[Paper]</a>
                            <a href="https://x.com/vincesitzmann/status/1801780132656324787">[Twitter Thread]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="imgs/flowmap_teaser.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent
                        </div>
                        <div class='paper-desc'>
                            arXiv
                        </div>
                        <div class='paper-authors'>
                            Cameron Smith*, David Charatan*, Ayush Tewari, Vincent Sitzmann
                        </div>
                        <div>
                            <a href="https://cameronosmith.github.io/flowmap/">[Project page]</a>
                            <a href="https://github.com/dcharatan/flowmap">[Code]</a>
                            <a href="https://arxiv.org/abs/2404.15259">[Paper]</a>
                            <a href="https://twitter.com/vincesitzmann/status/1783166394634575931">[Twitter Thread]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="imgs/pixelsplat_teaser.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction
                        </div>
                        <div class='paper-desc'>
                            CVPR 2024 (Oral, Best Paper Runner-Up)
                        </div>
                        <div class='paper-authors'>
                            David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann
                        </div>
                        <div>
                            <a href="https://davidcharatan.com/pixelsplat/">[Project page]</a>
                            <a href="https://github.com/dcharatan/pixelsplat">[Code]</a>
                            <a href="https://arxiv.org/abs/2312.12337">[Paper]</a>
                            <a href="https://twitter.com/vincesitzmann/status/1737630706325409996">[Twitter Thread]</a>
                        </div>
                    </div>
                </div>


                <div class='row vspace-top'>
                    <div class="col-sm-3">
                       <img src='imgs/barycentric_coords.png' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Variational Barycentric Coordinates
                        </div>
                        <div class='paper-desc'>
                            SIGGRAPH Asia 2023 (Journal Track)
                        </div>
                        <div class='paper-authors'>
                            Ana Dodik, Oded Stein, Vincent Sitzmann, Justin Solomon
                        </div>
                        <div>
                            <a href="https://anadodik.github.io/publication/vbc/vbc.pdf">[Paper]</a>
                            <a href="https://twitter.com/ana_dodik/status/1722740042609877043">[Twitter Thread]</a>
                        </div>
                    </div>
                </div>


                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="imgs/diffusion_teaser.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision
                        </div>
                        <div class='paper-desc'>
                            NeurIPS 2024 (Spotlight)
                        </div>
                        <div class='paper-authors'>
                            Ayush Tewari*, Tianwei Yin*, George Cazenavette, Joshua B. Tenenbaum, Fredo Durand, William T. Freeman, <u>Vincent Sitzmann</u>
                        </div>
                        <div>
                            <a href="https://diffusion-with-forward-models.github.io/">[Project page]</a>
                            <a href="https://github.com/ayushtewari/DFM/">[Code]</a>
                            <a href="https://diffusion-with-forward-models.github.io/diffusion-forward-paper.pdf">[Paper]</a>
                            <a href="https://twitter.com/vincesitzmann/status/1695111279588037088">[Twitter Thread]</a>
                        </div>
                    </div>
                </div>


                <!--- next ---!>
                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="imgs/flowcam_teaser.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                             FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow
                        </div>
                        <div class='paper-desc'>
                            NeurIPS 2024
                        </div>
                        <div class='paper-authors'>
                            Cameron Smith, Yilun Du, Ayush Tewari, Vincent Sitzmann
                        </div>
                        <div>
                            <a href="https://cameronosmith.github.io/flowcam/">[Project page]</a>
                            <a href="https://arxiv.org/abs/2306.00180">[Paper]</a>
                            <a href="https://github.com/cameronosmith/FlowCam">[Code]</a>
                        </div>
                    </div>
                </div>


                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="imgs/stereo-view-synthesis-teaser.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Learning to Render Novel Views from Wide-Baseline Stereo Pairs
                        </div>
                        <div class='paper-desc'>
                            CVPR 2023
                        </div>
                        <div class='paper-authors'>
                            Yilun Du, Cameron Smith, Ayush Tewari†, Vincent Sitzmann†
                        </div>
                        <div>
                            <a href="https://yilundu.github.io/wide_baseline/">[Project page]</a>
                            <a href="https://arxiv.org/abs/2304.08463">[Paper]</a>
                            <a href="https://github.com/yilundu/cross_attention_renderer">[Code]</a>
                            <a href="https://colab.research.google.com/drive/1PeL5oJ_eraLEdzTEVPLBwoM2pyv26WcU?usp=sharing">[Colab]</a>
                        </div>
                    </div>
                </div>


                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="groundplans/thumbnail.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Seeing 3D Objects in a Single Image via Self-Supervised Static-Dynamic Disentanglement
                        </div>
                        <div class='paper-desc'>
                            ICLR 2022
                        </div>
                        <div class='paper-authors'>
                            Prafull Sharma, Ayush Tewari, Yilun Du, Sergey Zakharov, Rares Ambrus, Adrien Gaidon,
                            William T. Freeman, Fredo Durand, Joshua B. Tenenbaum, <u>Vincent Sitzmann</u>
                        </div>
                        <div>
                            <a href="https://prafullsharma.net/see3d/">[Project page]</a>
                            <a href="https://prafullsharma.net/see3d/paper.pdf">[Paper]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="feature-fields/thumbnail.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                             Decomposing NeRF for Editing via Feature Field Distillation
                        </div>
                        <div class='paper-desc'>
                            NeurIPS 2022
                        </div>
                        <div class='paper-authors'>
                            Sosuke Kobayashi, Eiichi Matsumoto, <u>Vincent Sitzmann</u>
                        </div>
                        <div>
                            <a href="https://pfnet-research.github.io/distilled-feature-fields/">[Project page]</a>
                            <a href="https://arxiv.org/abs/2205.15585">[Paper]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="ndfs/thumbnail.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation
                        </div>
                        <div class='paper-desc'>
                            ICRA 2022
                        </div>
                        <div class='paper-authors'>
                            Anthony Simeonov*, Yilun Du*, Andrea Tagliasacchi, Alberto Rodriguez, Pulkit Agrawal†, <u>Vincent Sitzmann</u>†
                        </div>
                        <div>
                            <a href="https://yilundu.github.io/ndf/">[Project page]</a>
                            <a href="https://arxiv.org/abs/2112.05124">[Paper]</a>
                            <a href="https://github.com/anthonysimeonov/ndf_robot">[Code]</a>
                            <a href="https://colab.research.google.com/drive/16bFIFq_E8mnAVwZ_V2qQiKp4x4D0n1sG?usp=sharing">[Colab]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="imgs/gem_thumbnail.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Learning Signal-Agnostic Manifolds of Neural Fields
                        </div>
                        <div class='paper-desc'>
                            NeurIPS 2021
                        </div>
                        <div class='paper-authors'>
                            Yilun Du, Katherine M. Collins, Joshua Tenenbaum, <u>Vincent Sitzmann</u>
                        </div>
                        <div>
                            <a href="https://yilundu.github.io/gem/">[Project page]</a>
                            <a href="https://arxiv.org/abs/2111.06387">[Paper]</a>
                            <a href="https://github.com/yilundu/gem">[Code]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="lfns/img/rooms_360_compressed.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering
                        </div>
                        <div class='paper-desc'>
                            NeurIPS 2021 (Spotlight)
                        </div>
                        <div class='paper-authors'>
                            <u>Vincent Sitzmann</u>*, Semon Rezchikov*, William T. Freeman, Joshua B. Tenenbaum, Frédo Durand
                        </div>
                        <div>
                            <a href="https://vsitzmann.github.io/lfns">[Project page]</a>
                            <a href=" http://arxiv.org/abs/2106.02634">[Paper]</a>
                            <a href="https://github.com/vsitzmann/light-field-networks">[Code]</a>
<!--                            <a href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">[Colab]</a>-->
                        </div>
                    </div>
                </div>

<!--                <div class='row vspace-top'>-->
<!--                    <div class="col-sm-3">-->
<!--                        <img src='imgs/deep_medial_fields.png' class='img-fluid'>-->
<!--                    </div>-->

<!--                    <div class="col">-->
<!--                        <div class='paper-title'>-->
<!--                            Deep Medial Fields-->
<!--                        </div>-->
<!--                        <div class='paper-desc'>-->
<!--                            arXiv-->
<!--                        </div>-->
<!--                        <div class='paper-authors'>-->
<!--                            Daniel Rebain, Ke Li, <u>Vincent Sitzmann</u>, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi-->
<!--                        </div>-->
<!--                        <div>-->
<!--                            <a href="https://arxiv.org/abs/2106.03804">[Paper]</a>-->
<!--                        </div>-->
<!--                    </div>-->
<!--                </div>-->

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="siren/img/poisson_convergence_15s_label.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Implicit Neural Representations with Periodic Activation Functions
                        </div>
                        <div class='paper-desc'>
			    NeurIPS 2020 (Oral)
                        </div>
                        <div class='paper-authors'>
                            <u>Vincent Sitzmann</u>*, Julien N. P. Martel*, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein
                        </div>
                        <div>
                            <a href="https://vsitzmann.github.io/siren">[Project page]</a>
                            <a href="https://arxiv.org/abs/2006.09661">[Paper]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="metasdf/img/metasdf_steps_comp.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            MetaSDF: Meta-learning Signed Distance Functions
                        </div>
                        <div class='paper-desc'>
                            NeurIPS 2020
                        </div>
                        <div class='paper-authors'>
                            <u>Vincent Sitzmann</u>*, Eric R. Chan*, Richard Tucker, Noah Snavely, Gordon Wetzstein
                        </div>
                        <div>
                            <a href="https://vsitzmann.github.io/metasdf">[Project page]</a>
                            <a href="https://github.com/vsitzmann/metasdf">[Code]</a>
                            <a href="https://arxiv.org/abs/2006.09662">[Paper]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/star_img.png' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
			    State of the Art on Neural Rendering
                        </div>
                        <div class='paper-desc'>
				Computer Graphics Forum 2020 - EG 2020 (STAR Report)
                        </div>
                        <div class='paper-authors'>
                            Ayush Tewari*, Ohad Fried*, Justus Thies*, <u>Vincent Sitzmann*</u>, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nießner, Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B Goldman, Michael Zollhöfer
                        </div>
                        <div>
                            <a href="https://arxiv.org/pdf/2004.03805.pdf">[Paper]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/srn_seg_repimage.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
		            Inferring Semantic Information with 3D Neural Scene Representations
                        </div>
                        <div class='paper-desc'>
				3DV
                        </div>
                        <div class='paper-authors'>
                            Amit Kohli*, <u>Vincent Sitzmann*</u>, Gordon Wetzstein
                        </div>
                        <div>
                            <a href="https://www.computationalimaging.org/publications/semantic-srn/">[Project page]</a>
                            <a href="https://arxiv.org/pdf/2003.12673.pdf">[Preprint]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/SRNs.gif' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations
                        </div>
                        <div class='paper-desc'>
			    NeurIPS 2019 (Oral, <b>Honorable Mention "Outstanding New Directions"</b>)
                        </div>
                        <div class='paper-authors'>
                            <u>Vincent Sitzmann</u>, Michael Zollhöfer, Gordon Wetzstein
                        </div>
                        <div>
                            <a href="http://vsitzmann.github.io/srns/">[Project page]</a>
                            <a href="http://arxiv.org/abs/1906.01618">[Preprint]</a>
			    <a href="https://github.com/vsitzmann/scene-representation-networks">[Code]</a>
                        </div>
                    </div>
                </div>


                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/deepvoxels.png' class='img-fluid'>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                          DeepVoxels: Learning Persistent 3D Feature Embeddings
                      </div>
                      <div class='paper-desc'>
		      CVPR 2019 (Oral)
                      </div>
                      <div class='paper-authors'>
                      <u>Vincent Sitzmann</u>, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, Michael Zollhöfer
                      </div>
                      <div>
                         <a href="http://vsitzmann.github.io/deepvoxels/">[Project page]</a>
                         <a href="https://arxiv.org/abs/1812.01024">[Paper]</a>
                         <a href="https://github.com/vsitzmann/deepvoxels">[Code]</a>
                      </div>
                    </div>
                </div>


                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/onn_thumbnail.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                          Hybrid optical-electronic convolutional neural networks with optimized diffractive optics for image classification
                      </div>
                      <div class='paper-desc'>
                        Scientific Reports
                      </div>
                      <div class='paper-authors'>
                      Julie Chang, <u>Vincent Sitzmann</u>, Xiong Dun, Wolfgang Heidrich, Gordon Wetzstein
                      </div>
                      <div>
                        <a href="https://www.nature.com/articles/s41598-018-30619-y">[Paper]</a>
                      </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/end-to-end-cam.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                          End-to-end Optimization of Optics and Image Processing for Achromatic Extended Depth of Field and Super-resolution Imaging
                      </div>
                      <div class='paper-desc'>
                          SIGGRAPH 2018
                      </div>
                      <div class='paper-authors'>
                          <u>Vincent Sitzmann*</u>, Steven Diamond*, Yifan Peng*, Xiong Dun, Stephen Boyd, Wolfgang Heidrich, Felix Heide, Gordon Wetzstein
                      </div>
                      <div>
                         <a href="https://vsitzmann.github.io/deepoptics/">[Project page]</a>
                         <a href="https://drive.google.com/file/d/1Xums2qyqSGP_z_24HnYpM9gbRm1_uAzY/view?usp=sharing">[Paper]</a>
                         <a href="https://github.com/vsitzmann/deepoptics">[Code]</a>
                      </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/saliency.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                        Saliency in VR: How do people explore virtual environments?
                      </div>
                      <div class='paper-desc'>
                        IEEE VR 2018
                      </div>
                      <div class='paper-authors'>
                          <u>Vincent Sitzmann*</u>, Ana Serrano*, Amy Pavel, Maneesh Agrawala, Belen Masia, Diego Gutierrez, Gordon Wetzstein
                      </div>
                      <div>
                        <a href="https://vsitzmann.github.io/vr-saliency">[Project page]</a>
                        <a href="http://ieeexplore.ieee.org/document/8269807/">[Paper]</a>
                        <a href="https://www.github.com/vsitzmann/vr-saliency">[Code]</a>
                      </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/cognitive_events.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                          Movie Editing and Cognitive Event Segmentation in Virtual Reality Video
                      </div>
                      <div class='paper-desc'>
                          SIGGRAPH 2017
                      </div>
                      <div class='paper-authors'>
                          Ana Serrano, <u>Vincent Sitzmann</u>, Jaime Ruiz-Borau, Gordon Wetzstein, Diego Gutierrez, Belen Masia
                      </div>
                      <div>
                        <a href="http://webdiis.unizar.es/~aserrano/docs/Serrano_SIGG2017_VR-cine.pdf">[Paper]</a>
                      </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/sickness_prediction.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                          Towards a Machine-learning Approach for Sickness Prediction in 360° Stereoscopic Videos
                      </div>
                      <div class='paper-desc'>
                          IEEE VR 2018
                      </div>
                      <div class='paper-authors'>
                          Nitish Padmanaban*, Timon Ruban*, <u>Vincent Sitzmann</u>, Anthony M. Norcia, Gordon Wetzstein
                      </div>
                      <div>
                        <a href="http://ieeexplore.ieee.org/document/8267239/">[Paper]</a>
                      </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <div id='footer' class='vspace-top'>
    <div>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
</body>

</html>
